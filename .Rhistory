# @ensIdx: Index of the ensemble dimension
varName <- getVarName(nc)
# append indexes to the name
shortVarName <- paste(shortNames[[varName]], '_', paste(latIdx, lonIdx, fhourIdx, ensIdx, sep="."), sep="")
# build inputs to ncvar_get
startIdx <- c(lonIdx, latIdx, fhourIdx, ensIdx, 1)
cnt <- c(1, 1, 1, 1, -1) # -1 on intTime to get all values
values <- ncvar_get(nc, varName, start=startIdx, count=cnt)
dates <- ncvar_get(nc, 'intTime')
# build result data frame
res <- data.frame(date=dates)
res[shortVarName] <- values
return(res)
}
getAllVarData <- function(nc, lonIdx, latIdx) {
# For a given GEFS location and variable file, get and collapse all data
# Returns a dataframe where columns are the variables at different combinations of
# ensemble and forecast hour
# so should have a total of 5 * 11 = 55 columns
dims <- getDimensions(nc)
tmp <- list()
k <- 1
for (i in 1:length(dims$fhour)) {
for (j in 1:length(dims$ens)) {
tmp[[k]] <- getVarData(nc, lonIdx, latIdx, i, j)
k <- k + 1
}
}
# Profiling shows that the join all is really slow, so instead we'll just copy the columns, since
# in theory everything should be the same size.
#dframe <- join_all(tmp, by='date')
# Not using join_all reduces the processing time by 1/3
dates <- data.frame(date=tmp[[1]][,1])
colNames <- unlist(lapply(tmp, function(x) {return(colnames(x)[2])}))
values <- data.frame(do.call(cbind, lapply(tmp, function(x) {return(x[,2])})))
colnames(values) <- colNames
dframe <- cbind(dates, values)
if (sum(colnames(dframe) != 'date') != 55) {
warning(paste("Flattened data does not contain 55 columns in file ", nc$filename, sep=""))
}
return(dframe)
}
combineHours <- function(df, method="mean") {
# given a df that is a result of getAllVarData, reduces dimension of hours to 1
#
# For example, df will have column names with suffixes 1111, 1112, 1113 ... 1121, 1122... etc.
# Forecast hour is the third number, so this function will combine columns:
# 1111, 1121, 1131, 1141, 1151 to be 1101, and then the same with 1112, 1122, etc.
#
# Since getAllVarData has fixed lat and lon, we actually only need to flatten for the ensemble dimension
if (!(method %in% c('mean', 'sum', 'max', 'min'))) {
stop("Method is not valid")
}
# Get the suffixes of each column name
# Split first by _, then split by .
splitNames <- strsplit(colnames(df), '_')
suffixes <- unlist(lapply(splitNames, function(x) {return(x[2])}))
# This gives a list of numeric vectors, which indicate the dimensions of lat, lon, hour, and ensemble for each column of the df
# Also element 1 of the list is NA since that was the date column
idx <- strsplit(suffixes, '\\.')
# Additionally, infer a few things about this data.frame from the names
varName <- splitNames[[2]][1]
lonIdx <- substr(splitNames[[2]][2], 1, 1)
latIdx <- substr(splitNames[[2]][2], 3, 3)
ens <- as.numeric(unlist(lapply(idx, function(x) {x[4]})))
uniqueEns <- unique(ens[!is.na(ens)])
res <- data.frame(date=df$date)
for (e in uniqueEns) {
# The columns to merge
colIdx <- which(ens == e)
newCol <- apply(df[,colIdx], 1, get(method))
# add the new column to the res data frame
newName <- paste(varName, '_', paste(lonIdx, latIdx, 0, e, sep="."), sep="")
res[,newName] <- newCol
}
return(res)
}
combineEns <- function(df, method='mean') {
# Basically exactly the same as combineHours, but for combining across ensembles
if (!(method %in% c('mean', 'sum', 'max', 'min'))) {
stop("Method is not valid")
}
# Get the suffixes of each column name
# Split first by _, then split by .
splitNames <- strsplit(colnames(df), '_')
suffixes <- unlist(lapply(splitNames, function(x) {return(x[2])}))
# This gives a list of numeric vectors, which indicate the dimensions of lat, lon, hour, and ensemble for each column of the df
# Also element 1 of the list is NA since that was the date column
idx <- strsplit(suffixes, '\\.')
# Additionally, infer a few things about this data.frame from the names
varName <- splitNames[[2]][1]
lonIdx <- substr(splitNames[[2]][2], 1, 1)
latIdx <- substr(splitNames[[2]][2], 3, 3)
hrs <- as.numeric(unlist(lapply(idx, function(x) {x[3]})))
uniqueHrs <- unique(hrs[!is.na(hrs)])
res <- data.frame(date=df$date)
for (hr in uniqueHrs) {
# The columns to merge
colIdx <- which(hrs == hr)
newCol <- apply(df[,colIdx], 1, get(method))
# add the new column to the res data frame
newName <- paste(varName, '_', paste(lonIdx, latIdx, hr, 0, sep="."), sep="")
res[,newName] <- newCol
}
return(res)
}
##########
# Load data
##########
# Libraries ncdf4 and RNetCDF have basically the same functionality
library(ncdf4)
library(doMC)
library(foreach)
registerDoMC(cores=detectCores())
#singleStationData <- function(stn, dims) {
#cat('Cleaning data for station', stn, '\n')
#stnInfo <- stationInfo[stationInfo$stid == stn,]
## get four closest GEFS locations
#gefsLocs <- getPoints(stnInfo$elon, stnInfo$nlat, dims)
#latIdx <- seq(gefsLocs$latStart, gefsLocs$latStart + gefsLocs$latCnt - 1)
#lonIdx <- seq(gefsLocs$lonStart, gefsLocs$lonStart + gefsLocs$lonCnt - 1)
#allData <- list()
#i <- 1
#for (f in trainFiles[1:4]) {
#cat('Opening ', f, '\n', sep='')
#nc <- nc_open(paste0(dataFolder, trainFolder, f))
#for (lat in latIdx) {
#for (lon in lonIdx) {
#dtmp <- getAllVarData(nc, lon, lat)
## flatten ensembles
#allData[[i]] <- combineEns(dtmp)
#i <- i+1
#}
#}
#nc_close(nc)
#}
#dates <- data.frame(date=allData[[1]][,1])
#colNames <- unlist(lapply(allData, function(x) {return(colnames(x)[-1])}))
#values <- data.frame(do.call(cbind, lapply(allData, function(x) {return(x[,-1])})))
#colnames(values) <- colNames
#allData <- cbind(dates, values)
#return(allData)
#}
unlistData <- function(allData) {
# Takes a list of data frames and combines them.  Expects all data frames to have a date column as the first column.
# also all data frames should be the same number of rows.
dates <- data.frame(date=allData[[1]][,1])
colNames <- unlist(lapply(allData, function(x) {return(colnames(x)[-1])}))
values <- data.frame(do.call(cbind, lapply(allData, function(x) {return(x[,-1])})))
colnames(values) <- colNames
allData <- cbind(dates, values)
return(allData)
}
parSingleStationData <- function(stn, dims, train=TRUE) {
#tcolc_eatm	Total column-integrated condensate over the entire atmos.	kg m-2
#dswrf_sfc	Downward short-wave radiative flux average at the surface	W m-2
# Parallelized version of a single station data
cat('Cleaning data for station', stn, '\n')
if (train) {
subFolder <- trainFolder
fileNames <- trainFiles
} else {
subFolder <- testFolder
fileNames <- testFiles
}
stnInfo <- stationInfo[stationInfo$stid == stn,]
# get four closest GEFS locations
gefsLocs <- getPoints(stnInfo$elon, stnInfo$nlat, dims)
latIdx <- seq(gefsLocs$latStart, gefsLocs$latStart + gefsLocs$latCnt - 1)
lonIdx <- seq(gefsLocs$lonStart, gefsLocs$lonStart + gefsLocs$lonCnt - 1)
allData <- foreach(f=fileNames) %dopar% {
fileData <- list()
i <- 1
cat('Opening ', f, '\n', sep='')
nc <- nc_open(paste0(dataFolder, subFolder, f))
for (lat in latIdx) {
for (lon in lonIdx) {
dtmp <- getAllVarData(nc, lon, lat)
# flatten ensembles
fileData[[i]] <- combineEns(dtmp)
i <- i+1
}
}
nc_close(nc)
return(unlistData(fileData))
}
return(unlistData(allData))
}
buildDfs <- function(dfPath = 'cleaned/', train=TRUE) {
# For each Mesonet location:
## Find the four nearest GEFS Locations
## For each variable we want to include in the model
### For each GEFS location
#### Average over ensembles
#### average over prediction hours
#### Results in one column, add to dataset
## write df to folder
if (train) {
subFolder <- trainFolder
fileNames <- trainFiles
} else {
subFolder <- testFolder
fileNames <- testFiles
}
# Read in base dimension information
nc <- nc_open(paste0(dataFolder, subFolder, fileNames[1]))
dims <- getDimensions(nc)
nc_close(nc)
for (stn in stationNames) {
allData <- parSingleStationData(stn, dims, train=train)
#allData <- join_all(allData, by="date")
cat('Writing data frame of ', ncol(allData) - 1, ' columns\n', sep="")
fn <- paste0(dataFolder, dfPath, stn, '.RData')
save(allData, file = fn)
}
}
########
# Fit Model
########
library(caret)
if (FALSE) {
acme <- read.csv('../data/cleaned/ACME.csv')
acmeY <- trainData$ACME
acmeDf <- data.frame(y=acmeY, acme[,c(-1,-2)])
set.seed(998)
fitCtrl <- trainControl(method = "cv",
number = 5)
acmeFit <- train(y ~ .,
data=acmeDf,
method="rf",
trControl=fitCtrl,
verbose=TRUE,
ntree=100,
do.trace=TRUE)
library(randomForest)
acmeRf1 <- randomForest(y~., data=acmeDf[,1:21], do.trace=TRUE)
}
library(randomForest)
stationFit <- function(stn) {
cat('Fitting model for', stn, '\n', sep=' ')
trainingPath <- '../data/cleaned/'
f <- paste0(trainingPath, stn, '.RData')
values <- get(load(f))
values <- values[, -1]
#perform PCA
pcaFit = princomp(values)
#     values = pcaFit$scores[,1:10]
y <- trainData[,stn]
trainDf <- data.frame(y=y, values)
fitCtrl <- trainControl(method = "cv",
number = 5)
#     stnFit <- train(y ~ .,
#                     data=trainDf,
#                     method="rf",
#                     trControl=fitCtrl,
#                     verbose=TRUE,
#                     ntree=100)
stnFit <- randomForest(y~., data=trainDf, do.trace=TRUE, mtry=10, ntree=500)
print(stnFit)
#     save(stnFit, pcaFit, file=paste0('../data/models/', stn, '.Rdata'))
}
PCA <- function(stn){
trainingPath <- '../data/cleaned/'
f <- paste0(trainingPath, stn, '.RData')
DF = get(load(f))
res = princomp(DF[,-1])
print(sum(res$sdev[1:10]^2) / sum(res$sdev^2))
}
for (s in stationNames[1:5]) {
stationFit(s)
}
stationFit <- function(stn) {
cat('Fitting model for', stn, '\n', sep=' ')
trainingPath <- '../data/cleaned/'
f <- paste0(trainingPath, stn, '.RData')
values <- get(load(f))
values <- values[, -1]
#perform PCA
pcaFit = princomp(values)
#     values = pcaFit$scores[,1:10]
y <- trainData[,stn]
trainDf <- data.frame(y=y, values)
fitCtrl <- trainControl(method = "cv",
number = 5)
#     stnFit <- train(y ~ .,
#                     data=trainDf,
#                     method="rf",
#                     trControl=fitCtrl,
#                     verbose=TRUE,
#                     ntree=100)
stnFit <- randomForest(y~., data=trainDf, do.trace=TRUE, mtry=10, ntree=300)
print(stnFit)
save(stnFit, pcaFit, file=paste0('../data/models/', stn, '.Rdata'))
}
foreach(s=stationNames) %dopar% {
stationFit(s)
return(NULL)
}
predDf <- list()
i <- 1
for (s  in stationNames) {
cat('Prediction for station ', s, '\n', sep='')
predDf[[i]] <- predictStation(s)
i <- i + 1
}
res <- join_all(predDf, by="date")
write.csv(res, file = "submission.csv")
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
svmFit(stationNames[2])
get(wd)
get(wd)
getwd()
setwd("./Michael/")
svmFit(stationNames[2])
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
svmFit(stationNames[2])
registerDoMC(cores=detectCores())
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
svmFit(stationNames[2])
source('1_load.r')
setwd("../")
getwd()
source('1_load.r')
dataFolder <- 'data/'
testFolder <- 'test/' # Path to test files.  Set to '' if in root dataFolder
trainFolder <- 'train/' # Path to train files
trainOutFolder <- 'cleaned/' # output of reshaped training data
testOutFolder <- 'cleanedTest/' # output of reshaped test data
modelsFolder <- 'models/'
source('1_load.r')
source('2_func.r')
registerDoMC(cores=detectCores())
stationNames
sample(stationNames, 10)
predDf <- list()
i <- 1
for (s  in stationNames) {
cat('Prediction for station ', s, '\n', sep='')
predDf[[i]] <- predictStation(s)
i <- i + 1
}
res <- join_all(predDf, by="date")
write.csv(res, file = "submission.csv", row.names=FALSE)
predDf <- list()
i <- 1
for (s  in stationNames) {
cat('Prediction for station ', s, '\n', sep='')
predDf[[i]] <- predictStation(s)
i <- i + 1
}
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
predDf <- list()
i <- 1
for (s  in stationNames) {
cat('Prediction for station ', s, '\n', sep='')
predDf[[i]] <- predictStation(s)
i <- i + 1
}
getwd()
res <- join_all(predDf, by="date")
write.csv(res, file = "submission.csv", row.names=FALSE)
source('~/Documents/R/Kaggle - ams-solar/1_load.r')
source('~/Documents/R/Kaggle - ams-solar/1_load.r')
buildDfs()
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
##########
# Config
##########
dataFolder <- 'data/'
testFolder <- 'test/' # Path to test files.  Set to '' if in root dataFolder
trainFolder <- 'train/' # Path to train files
trainOutFolder <- 'cleaned/' # output of reshaped training data
testOutFolder <- 'cleanedTest/' # output of reshaped test data
modelsFolder <- 'models/'
source('1_load.r')
source('2_func.r')
registerDoMC(cores=detectCores())
buildDfs()
stationFit("ACME")
get(wd)
getwd()
trainGrid = expand.grid(.mtry = seq(2:100, 20))
trainGrid = expand.grid(.mtry = seq(2,100, 20))
trainGrid
createGrid("rf", len = 5)
load("data/cleaned/ACME.RData")
createGrid("rf", len = 5, data = allData[,-1])
trainGrid = expand.grid(.mtry = seq(2,102, 20))
trainGrid
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
##########
# Config
##########
dataFolder <- 'data/'
testFolder <- 'test/' # Path to test files.  Set to '' if in root dataFolder
trainFolder <- 'train/' # Path to train files
trainOutFolder <- 'cleaned/' # output of reshaped training data
testOutFolder <- 'cleanedTest/' # output of reshaped test data
modelsFolder <- 'models/'
source('1_load.r')
source('2_func.r')
registerDoMC(cores=detectCores())
stationFit("ACME")
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
source('~/Documents/R/Kaggle - ams-solar/1_load.r')
buildDfs(train=TRUE)
buildDfs(train=FALSE)
foreach(s=stationNames) %dopar% {
stationFit(s)
return(NULL)
}
stationFit("ACME")
getwd()
stationNames
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
mdl
str(mdl)
mdl$rsq
mdl
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
stationInfo
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
warnings
warnings()
stationPerf
source('~/.active-rstudio-document')
stationPerf
stationPerf = data.frame(matrix(0, 98, 4))
stationPerf
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
stationPerf
source('~/.active-rstudio-document')
stationPerf
str(stationPerf)
as.numeric(stationPerf[,-1])
as.matrix(stationPerf[,-1])
as.numeric(stationPerf[,-1])
stationFit("ACME")
stationFit("ACME")
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
stationFit("ACME")
plot(stnFit)
plot(log(stnFit))
Q
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
stationNames
stationFit("APAC")
getwd()
traceback()
stationFit("ACME")
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
##########
# Config
##########
dataFolder <- 'data/'
testFolder <- 'test/' # Path to test files.  Set to '' if in root dataFolder
trainFolder <- 'train/' # Path to train files
trainOutFolder <- 'cleaned/' # output of reshaped training data
testOutFolder <- 'cleanedTest/' # output of reshaped test data
modelsFolder <- 'models/'
source('1_load.r')
source('2_func.r')
registerDoMC(cores=detectCores())
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
stationFit("ACME")
setwd()
getwd()
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
stationFit("ACME")
print(stnFit)
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
system.time(stationFit("ACME"))
foreach(s=stationNames) %dopar% {
stationFit(s)
return(NULL)
}
predDf <- list()
i <- 1
for (s  in stationNames) {
cat('Prediction for station ', s, '\n', sep='')
predDf[[i]] <- predictStation(s)
i <- i + 1
}
source('~/Documents/R/Kaggle - ams-solar/2_func.r')
predDf <- list()
i <- 1
for (s  in stationNames) {
cat('Prediction for station ', s, '\n', sep='')
predDf[[i]] <- predictStation(s)
i <- i + 1
}
res <- join_all(predDf, by="date")
write.csv(res, file = "submission.csv", row.names=FALSE)
plot(res[,2])
